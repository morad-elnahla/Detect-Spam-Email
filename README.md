# ğŸ“© Spam Detection Project

**Notebook:** `/mnt/data/Spam.ipynb`

---

## ğŸš€ Overview
Machine learning model for classifying messages into **Spam** or **Ham**.  
Includes: data loading â†’ text processing â†’ training â†’ evaluation â†’ saving model.

---

## ğŸ—‚ Dataset
Columns renamed:
- `v1` â **Target**
- `v2` â **Text**

Dataset contains:
- **Text:** message content  
- **Target:** spam / ham  

---

## ğŸ§¹ Preprocessing Steps
- ğŸ”¤ Lowercasing  
- âœ‚ï¸ Removing punctuation  
- ğŸ§© Tokenization (NLTK)  
- ğŸ›‘ Removing stopwords  
- ğŸ§® Vectorization (TF-IDF / CountVectorizer)  
- âœ‚ï¸ Train/Test Split  

---

## ğŸ¤– Models Used
Sklearn models detected:
- Logistic Regression  
- Multinomial Naive Bayes  
- SVM / Random Forest  

(Training confirmed by `.fit()` calls)

---

## ğŸ“Š Evaluation
Metrics detected:
- âœ” Accuracy  
- âœ” Precision / Recall / F1  
- âœ” Confusion Matrix  
- âœ” Possibly ROC-AUC  

---

## ğŸ’¾ Model Saving
Model exported using:
- `joblib.dump()` **or**  
- `pickle.dump()`

---

## ğŸ§  What We Actually Did in This Project
In this notebook, we built a complete **end-to-end spam detection system**:

- Loaded the dataset and inspected its structure.  
- Cleaned the raw text (lowercase, removed punctuation, tokenized, removed stopwords).  
- Transformed messages into numerical vectors using **TF-IDF**.  
- Split the dataset into **training** and **testing** sets.  
- Trained multiple classic ML models (Naive Bayes, Logistic Regression, SVM, Random Forest).  
- Compared performance using accuracy, precision, recall, F1-score.  
- Identified the best-performing model (Naive Bayes typically performs best for text).  
- Saved the model so it can be reused for prediction or deployed in an app.  

This results in a fully working **Spam/Ham classifier** ready for real-world integration.

---

## ğŸ”¥ Next Steps
- Add actual metrics  
- Document saved model path  
- Create Streamlit app  
- Improve model with GridSearchCV  

---
